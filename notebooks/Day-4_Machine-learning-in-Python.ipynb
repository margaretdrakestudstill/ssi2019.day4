{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Day 4\n",
    "* Preliminaries\n",
    "* What even is machine learning?\n",
    "* Machine learning in Python: why scikit-learn?\n",
    "* Feature extraction\n",
    "* Feature selection\n",
    "* Estimation\n",
    "* Evaluation\n",
    "* Automation\n",
    "* A brief look at deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll load scikit-learn modules as we go,\n",
    "# so we can see what we're using.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Read in our preprocessed data set from Day 3.\n",
    "# You may need to modify the path to the file\n",
    "# depending on where you put it on your computer.\n",
    "data = pd.read_csv('../data/preprocessed_data.csv')\n",
    "\n",
    "# Since we'll be predicting outcomes, let's restrict\n",
    "# to only common ones. It's hard to predict something\n",
    "# we don't have very many training examples of.\n",
    "data = data.groupby('outcome').filter(lambda x: len(x) >= 500)\n",
    "\n",
    "# Let's also do some data cleanup to make life easier\n",
    "data = data.dropna(subset=['age'])\n",
    "categoricals = ['sex', 'sterilized']\n",
    "data[categoricals] = data[categoricals].fillna('Unknown')\n",
    "\n",
    "# Important, otherwise we have problems later\n",
    "# when we try to concatenate based on index\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What even is machine learning?\n",
    "<img src=\"https://s-media-cache-ak0.pinimg.com/736x/be/fc/cc/befcccae9891a505eabee71f7c808d4d.jpg\" style=\"margin-bottom: 10px;\">\n",
    "\n",
    "* Better name might be \"predictive modeling\"\n",
    "    * In contrast to traditional statistical approach, which might be called \"explanatory modeling\" [$^1$](https://projecteuclid.org/euclid.ss/1009213726),[$^2$](http://jakewestfall.org/publications/Yarkoni_Westfall_choosing_prediction.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Traditional approach begins by positing a _data model_\n",
    "    * e.g., linear regression model: $Y = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p + \\epsilon$\n",
    "    * ***\"Assuming my model of how the data arose is correct, and given these parameter estimates, to what extent is variation in Y explained by variation in X?\"***\n",
    "    * Models should be interpretable in order to be useful\n",
    "* In machine learning, overriding emphasis is just on whether we can accurately predict future Y values\n",
    "    * ***\"Regardless of what the true data model may be or what my parameter estimates look like, does my algorithm give outputs (Y) as close as possible to those of the true model when given the same inputs (X)?\"***\n",
    "    * If the model/algorithm is interpretable, that's a bonus, but not generally important\n",
    "    * Premium placed instead on objective tests of accuracy in predicting new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* How do we test for overfitting **before** the new data come in?\n",
    "* By setting aside a fraction of our dataset -- no peeking! -- and pretending it's the future data\n",
    "    * These set-aside data are called the *validation set*\n",
    "* Example: We have 1000 samples in the full dataset. \n",
    "    * Fit the model to 900 samples (the training set)\n",
    "    * Test how accurately the model predicts the remaining 100 samples (the validation set)\n",
    "* This is the basic logic. In practice we often use a more sophisticated version of this called *cross-validation*. We'll return to this idea near the end of the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning in Python: why scikit-learn?\n",
    "* There are a *lot* of ML packages in Python\n",
    "    * Theano, Tensorflow, orange, Pattern, PyMVPA, etc...\n",
    "* But for most applications, scikit-learn is dominant\n",
    "    * Elegant, powerful interface\n",
    "    * World-class [documentation](http://scikit-learn.org/stable/)\n",
    "    * Excellent performance\n",
    "* The main exception is deep learning--not supported in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The typical predictive modeling pipeline\n",
    "* Feature extraction/engineering\n",
    "    * A \"feature\" in ML terminology is a predictor/variable \n",
    "* Feature selection/dimensionality reduction\n",
    "* Model/parameter selection\n",
    "* Estimation\n",
    "* Evaluation\n",
    "* Rinse and repeat ad nauseam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature extraction\n",
    "* Deriving *new* (informative, non-redundant) predictors/features from the features you already have\n",
    "* Or adding entirely new features to the dataset (e.g., web scraping for dog breed stats)\n",
    "* Simple example of feature extraction: Adding a squared term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,6))\n",
    "\n",
    "# estimate a simple linear regression model: predict min_weight from min_height\n",
    "linear_model = sm.OLS(data['min_weight'], sm.add_constant(data['min_height']), missing='drop')\n",
    "\n",
    "# get the regression coefficients (intercept and slope)\n",
    "a, b = linear_model.fit().params\n",
    "\n",
    "# plot the results in the left panel\n",
    "axes[0].scatter(data['min_height'], data['min_weight'], s=30)\n",
    "axes[0].plot([0, 30], [a, a + b*30], lw=3)\n",
    "\n",
    "# Add labels and title\n",
    "axes[0].set(ylabel=\"Breed weight\", xlabel=\"Breed height\")\n",
    "axes[0].set_title(\"Weight = b0 + b1*Height\", size=20)\n",
    "\n",
    "# now add height-squared to the model\n",
    "quad_data = sm.add_constant(pd.DataFrame({'h':data['min_height'], 'h2':data['min_height']**2}))\n",
    "quadratic_model = sm.OLS(data['min_weight'], quad_data, missing='drop')\n",
    "a, b, c = quadratic_model.fit().params\n",
    "\n",
    "# plot quadratic fit in the right panel\n",
    "axes[1].scatter(data['min_height'], data['min_weight'], s=30)\n",
    "X = np.linspace(0, 30)\n",
    "axes[1].plot(X, [a + b*x + c*x**2 for x in X], lw=3)\n",
    "\n",
    "# Add labels and title\n",
    "axes[1].set(ylabel=\"Breed weight\", xlabel=\"Breed height\")\n",
    "axes[1].set_title(\"Weight = b0 + b1*Height + b2*Height^2\", size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting _all_ the features\n",
    "* Well, probably not all... but a _lot_\n",
    "* How much information can we get out of the original dataset?\n",
    "* From an interpretation-oriented standpoint, maybe not much more\n",
    "* From a machine learning standpoint, we've just scratched the surface\n",
    "* Some things we could add: names, colors, any number of interactions...\n",
    "    * The cost of trying out silly things is much lower\n",
    "    * Multicollinearity is not (much of) a concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The bag-of-words model\n",
    "* If we have a lot of data, it's not always worth thinking deeply about our features\n",
    "* E.g., how should we model fur color?\n",
    "    * \"Black/Tricolor\", \"Calico Point\", \"Brown Brindle/Blue Cream\"\n",
    "* Simple approach: treat color descriptions like a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "* Extract all word tokens (possibly even N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# How many colors in total?\n",
    "print(data['color'].nunique())\n",
    "\n",
    "# First 20 unique colors in the dataset\n",
    "data['color'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applying the bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The CountVectorizer is an estimator that takes a series\n",
    "# of documents (or strings) as input, and returns a count\n",
    "# of every word token found in every document. There's also\n",
    "# a TfidfVectorizer in cases where we want normalized frequency.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with default parameters.\n",
    "# Some common arguments we might want to experiment with\n",
    "# include min_df and max_df (which exclude words that are\n",
    "# too frequent or infrequent), stop_words (which allows\n",
    "# us to pass in a list of words to ignore), and ngram_range,\n",
    "# which enables us to extract multi-word features.\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Extract all possible word features from the color list.\n",
    "# Note that this returns a sparse matrix rather than a\n",
    "# numpy array or a pandas DataFrame. A sparse matrix is\n",
    "# a way of representing potentially very large 2-d arrays\n",
    "# very efficiently, because we don't need to allocate\n",
    "# memory for every cell in the array, only those that\n",
    "# have a non-zero value.\n",
    "fur_features = vec.fit_transform(data['color'])\n",
    "print(\"fur_features is an object of type:\", type(fur_features))\n",
    "\n",
    "# After fitting, the names of the features (i.e., the\n",
    "# columns of the sparse matrix returned by fit_transform())\n",
    "# are stored in the estimator itself.\n",
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "# Store in a pandas DF for easier manipulation later.\n",
    "# Note that we convert the sparse array back to a dense\n",
    "# one before loading into pandas. If our dataset were\n",
    "# much bigger, we'd probably want to avoid this step\n",
    "# and just keep working with the sparse matrix.\n",
    "fur_features = pd.DataFrame(fur_features.todense(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look...\n",
    "fur_features.head()\n",
    "# We went from 534 unique color combinations to 37 binary color features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# How many instances of each color?\n",
    "fur_features.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What about names?\n",
    "* At face value, it seems like we could include binary indicators for animal names\n",
    "* But this is probably pushing it a bit too far: there are 16,000 different names!\n",
    "* Are there ways we can capture potentially useful information with fewer variables?\n",
    "* Let's get creative...\n",
    "     * First letter of the name (~30 variables)\n",
    "     * Length of name (in characters)\n",
    "     * Number of words in the name\n",
    "     * Others?\n",
    "         * We're limited to things we can automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get the first letter of the name\n",
    "first_letter_of_name = data['name'].str.slice(0, 1)\n",
    "\n",
    "# Convert to lowercase to avoid typos, case differences, etc.\n",
    "first_letter_of_name = first_letter_of_name.str.lower()\n",
    "\n",
    "# What do we end up with?\n",
    "print(\"Unique first characters:\", first_letter_of_name.unique())\n",
    "\n",
    "# Convert to dummy variables\n",
    "name_features = pd.get_dummies(first_letter_of_name, prefix='first_char')\n",
    "\n",
    "# Let's see...\n",
    "name_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a few other features and add them to\n",
    "# the name_features DataFrame we built above\n",
    "\n",
    "\n",
    "# Number of characters in the name\n",
    "name_features['num_chars'] = data['name'].str.len()\n",
    "\n",
    "# Number of words: we search for the number of occurrences of\n",
    "# whitespace, then add 1 (because a single word will have no\n",
    "# whitespace)\n",
    "name_features['num_words'] = data['name'].str.count('\\s+') + 1\n",
    "\n",
    "# Anything else we want to add? Add code here...\n",
    "\n",
    "\n",
    "# We need to drop NaNs, because they'll make scikit-learn choke\n",
    "# when we turn to fitting our models. We'll take the easy way\n",
    "# out and just replace NaNs with 0's. This isn't ideal because\n",
    "# it changes the meaning of our variables; a better approach\n",
    "# might be to add a separate binary indicator coding whether\n",
    "# values are missing or not for each variable that has NaNs.\n",
    "name_features = name_features.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding interaction terms\n",
    "* We could dummy-code all our categorical variables and then take pairwise products\n",
    "* But if we don't need interpretabilty, there's a simpler hack\n",
    "    * Concatenate all variables for which we want interactions\n",
    "        * E.g., `breed='Alsatian'` and `sex='F'` becomes `Alsatian_F`\n",
    "    * Dummy-code the result\n",
    "    * A lightweight version of the [\"hashing trick\"](https://en.wikipedia.org/wiki/Feature_hashing)\n",
    "* Let's cross sex (2 categories), sterilization (2 categories), and breed (1787 categories)\n",
    "    * Could result in up to $2 \\times 2 \\times 1787 = 7148$ new features! But since many of these combinations of categories likely contain 0 pets, it'll probably be fewer than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Simply concatenate the columns we want--making sure to convert any numeric\n",
    "# columns to string, otherwise the concatenation will fail.\n",
    "data['ssb'] = data['sex'].astype(str) + '_' \\\n",
    "    + data['sterilized'].astype(str) + '_' + data['breed']\n",
    "\n",
    "# How many unique levels?\n",
    "num_levels = data['ssb'].nunique()\n",
    "print(\"Total number of unique values: {}\".format(num_levels))\n",
    "\n",
    "# Now we can dummy-code the result\n",
    "pd.get_dummies(data['ssb']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What else?\n",
    "* Very easy to quickly build up thousands of derivative features in this way\n",
    "* Doesn't mean we shouldn't think deeply about good features\n",
    "    * Often, biggest jumps in performance are achieved by adding entirely new features (e.g., external dog breed data)\n",
    "* Point is try to eke out every bit of signal from what we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection/reduction\n",
    "* Not all features are created equal\n",
    "* Just because we created 4,000+ features doesn't mean we need to include them all\n",
    "* Two general approaches:\n",
    "    1. Dimensionality reduction (extract latent signal from observed features)\n",
    "        * E.g., principal component analysis (PCA)\n",
    "    2. Feature selection (filter out features based on some criterion)\n",
    "        * Keep high-variance features\n",
    "        * Keep best-scoring features (i.e., strongest correlation with outcome)\n",
    "        * Fit a preliminary estimator like lasso that drops some features\n",
    "        * etc.\n",
    "* Supported by the `decomposition` and `feature_selection` modules in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principal Component Analysis (PCA): The big idea\n",
    "* For $P$ variables, we can plot the observations as a scatter plot in $P$-dimensional space\n",
    "    * E.g., for $P=2$ features, we have a two-dimensional scatter plot with two axes, the X-axis and the Y-axis\n",
    "* In PCA, we *rotate* the dataset in $P$-dimensional space to a *new* set of $P$ axes, called *principal components*, such that\n",
    "    1. The observations are uncorrelated along the principal component axes\n",
    "    2. The principal components are sorted in descending order of variance accounted for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the two features. It's usually \n",
    "# a good idea to do this prior to running PCA.\n",
    "# Notice that by only keeping data with min_height\n",
    "# and min_weight defined, we're implicitly restricting\n",
    "# our dataset to use only dogs.\n",
    "df = data[['min_height', 'min_weight']].dropna()\n",
    "df = (df - df.mean()) / df.std()\n",
    "\n",
    "# compute the PCA using scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df)\n",
    "\n",
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,7))\n",
    "\n",
    "# data points in original, unrotated features space\n",
    "axes[0].scatter(df['min_height'], df['min_weight'], s=30)\n",
    "axes[0].set_xlabel(\"Breed height\")\n",
    "axes[0].set_ylabel(\"Breed weight\")\n",
    "axes[0].set_ylim([-2, 3.5])\n",
    "axes[0].set_xlim([-2, 3.5])\n",
    "a, b = (0, pca.components_[0,1]/pca.components_[0,0])\n",
    "axes[0].plot([-1.8, 2.4], [a + b*-1.8, a + b*2.4], lw=3)\n",
    "a, b = (0, pca.components_[1,1]/pca.components_[1,0])\n",
    "axes[0].plot([-.9, .9], [a + b*-.9, a + b*.9], lw=3)\n",
    "\n",
    "# data points in rotated principal component (PC) space\n",
    "rotated = pca.transform(df)\n",
    "axes[1].scatter(-rotated[:,0], rotated[:,1], s=30)\n",
    "axes[1].set_xlabel(\"Principal Component 1\")\n",
    "axes[1].set_ylabel(\"Principal Component 2\")\n",
    "axes[1].set_ylim([-2.5, 4])\n",
    "axes[1].set_xlim([-2.5, 4])\n",
    "axes[1].plot([-2.7, 3.5], [0, 0], lw=3)\n",
    "axes[1].plot([0, 0], [-1.3, 1.3], lw=3);\n",
    "\n",
    "# How much of the total variance is explained by each PC?\n",
    "msg = \"The first principal component alone explains {:.0%} of the total variance.\"\n",
    "print(msg.format(pca.explained_variance_ratio_[0 ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It's clear that the first principal component contains most of the useful information that was shared between Height and Weight\n",
    "* So if we simply drop the second principal component, we've effectively reduced our two features down to one feature with minimal information loss\n",
    "    * In practice, this won't make much of a difference when we have 100k observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now let's do PCA on our array of over 4000 features\n",
    "* PCA on such a large array would be very computationally demanding and take a long time\n",
    "* So we'll do a faster, approximate version of PCA called *Randomized PCA*\n",
    "* We'll extract the first 100 components (a 40-fold reduction in dimensionality!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to recode our string column as a set of dummies\n",
    "interaction_dummies = pd.get_dummies(data['ssb'])\n",
    "\n",
    "# Let's concatenate this with the color features\n",
    "# axis=1 indicates that we want to concatenate along\n",
    "# the column axis (axis=0 would append each dataframe\n",
    "# below the last.\n",
    "lotsa_features = pd.concat([fur_features, interaction_dummies], axis=1)\n",
    "\n",
    "# Like most other things in sklearn, decomposition classes\n",
    "# implement the estimator interface. So they have fit() and\n",
    "# predict() methods. Transformers also have a transform()\n",
    "# method. First, we initialize the PCA transformer.\n",
    "# We'll use a \"randomized PCA\" solver that is a speedier\n",
    "# approximation of the standard principal component analysis\n",
    "# (PCA) factorization. We need to specify the number of\n",
    "# components we want at initialization. We'll take the first 100.\n",
    "from sklearn.decomposition import PCA\n",
    "rpca = PCA(100, svd_solver='randomized')\n",
    "\n",
    "# Now we can fit and transform in one step\n",
    "rpca_features = rpca.fit_transform(lotsa_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# How much of the variance do these components explain?\n",
    "\n",
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,7))\n",
    "\n",
    "# scree plot\n",
    "axes[0].plot(rpca.explained_variance_ratio_, lw=3)\n",
    "axes[0].set_title('Scree plot (% variance explained by each PC)', size=20)\n",
    "axes[0].set_xlabel('Principal Component number')\n",
    "axes[0].set_ylabel('% of total variance')\n",
    "\n",
    "# cumulative variance explained\n",
    "axes[1].plot(np.cumsum(rpca.explained_variance_ratio_), lw=3)\n",
    "axes[1].set_title('Cumulative variance explained', size=20)\n",
    "axes[1].set_xlabel('Principal Component number')\n",
    "axes[1].set_ylabel('% of total variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimation\n",
    "* With features in hand, we can now fit some models!\n",
    "* Our goal is to predict dogs' outcomes as best we can\n",
    "    * This is a *multi-class classification* problem\n",
    "    * The outcome variable has several discrete classes\n",
    "* scikit-learn has a bewildering array of models—where should we start?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "* A good baseline model: simple, but often performs very well\n",
    "* Can do multiclass classification (which we need!)—e.g., via the [one-vs-rest](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) strategy\n",
    "    * Effectively, we fit a separate classifier for each outcome\n",
    "* Let's see how this works in scikit-learn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Restrict the data to only dogs—outcome prediction is probably\n",
    "# sufficiently different for different species that it doesn't\n",
    "# make a lot of sense to lump them.\n",
    "dogs = data.query('animal == \"Dog\"')\n",
    "\n",
    "# The linear_model module contains a few dozen linear models\n",
    "# for classification and regression problems. We'll grab the\n",
    "# LogisticRegression classifier.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our logistic regression estimator.\n",
    "# We set multi_class to 'auto', which will default to\n",
    "# using a one-versus-rest approach.\n",
    "est = LogisticRegression(multi_class='auto')\n",
    "\n",
    "# Let's start with a really simple model: predicting\n",
    "# outcome from just the dog's age (plus an intercept,\n",
    "# which is added automatically).\n",
    "X = dogs[['age']]\n",
    "\n",
    "# # We need to convert the outcome string to integer\n",
    "# class labels. Fortunately, sklearn has a utility\n",
    "# for that. The LabelEncoder is also an estimator, so\n",
    "# we need to initialize it before we fit_transform.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(dogs['outcome'])\n",
    "\n",
    "# Fitting the model is as simple as passing\n",
    "# the X and y arrays to fit().\n",
    "est.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How did we do?\n",
    "* Our model's fitted! Let's see how accurately we classified outcomes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predict y from the trained model. Note that\n",
    "# scikit-learn will not do this for you automatically.\n",
    "# So we call predict(), passing in only the X this\n",
    "# time (since we've already fitted the model, and now\n",
    "# we're generating predictions for the X's).\n",
    "y_predicted = est.predict(X)\n",
    "\n",
    "# Now we have a variety of evaluation metrics.\n",
    "# We'll talk about those below. The simplest one is\n",
    "# to just look at the number of cases we classified\n",
    "# correctly.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Most scoring functions in metrics take the true\n",
    "# labels and the predicted labels, in that order.\n",
    "scores = accuracy_score(y, y_predicted)\n",
    "print(\"Overall classification accuracy: {:.0%}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Is this good?\n",
    "* Seems good, no?\n",
    "* There are 4 classes, so chance should be 25%, right?\n",
    "* Or should it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show the relative class frequencies\n",
    "counts = dogs['outcome'].value_counts()\n",
    "counts/counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can (probably) do better\n",
    "* We shouldn't expect to do very well with just one predictor\n",
    "* Let's use that giant mess of features we extracted!\n",
    "* We can experiment with some of the other classifiers available in scikit-learn\n",
    "* Important note: for some classifiers, it's going to look like we're doing *way* better than we really are\n",
    "    * We'll explain why shortly; just keep this in mind as we experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "\n",
    "# Some other estimators we can try (there are many more!)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB      # Naive Bayes for binary outcomes\n",
    "from sklearn.linear_model import RidgeClassifier # ridge regression + classification rule\n",
    "\n",
    "# grab the outcome and encode it as integer rather than string.\n",
    "# we already did this above, but let's do it again just to make\n",
    "# sure we remember what's happening.\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(data['outcome'])\n",
    "\n",
    "# plug in estimator of choice (just using default arguments).\n",
    "# there are plenty of other classifiers in sklearn you can\n",
    "# experiment with (remember to import them above)\n",
    "\n",
    "est = RidgeClassifier()\n",
    "# est = LogisticRegression()\n",
    "# est = BernoulliNB()\n",
    "# est = DecisionTreeClassifier()       # A single decision tree\n",
    "# est = RandomForestClassifier()       # A forest of decision trees\n",
    "\n",
    "# The following classifiers will choke on the full datasets;\n",
    "# if you want to experiment with them, you'll probably need\n",
    "# to cut down the number of features in X and/or the number\n",
    "# of rows in X and y.\n",
    "# est = KNeighborsClassifier() # default K = 5\n",
    "# est = SVC()\n",
    "\n",
    "# Set up the predictive features--i.e., the X matrix.\n",
    "# The np.c_ function concatenates arrays along their column\n",
    "# axis. There is also np.r_ for row-wise concatenation.\n",
    "# We could also have done this using np.concatenate().\n",
    "# Here we include as predictors dog age, the 100 PCA\n",
    "# components we extracted from the fur/sex/sterilized\n",
    "# variables, and the name features.\n",
    "X = np.c_[data['age'], rpca_features[:,:100], name_features]\n",
    "\n",
    "# Standardize the features: this will matter a lot for\n",
    "# some classifiers (e.g., ridge or lasso), so let's do it\n",
    "# for everything (it will almost never hurt us).\n",
    "X = scale(X)\n",
    "\n",
    "# fit the model/algorithm\n",
    "est.fit(X, y)\n",
    "\n",
    "# Generate predicted scores.\n",
    "y_predicted = est.predict(X)\n",
    "\n",
    "# We're assessing accuracy *in the training set* so be skeptical!\n",
    "accuracy_score(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Experiment away!\n",
    "* This is a good time to experiment with the model\n",
    "    * Use different estimators\n",
    "        * Be aware that some will be impractically slow with 50k rows\n",
    "    * Vary the parameters of the estimators\n",
    "    * Include different predictors\n",
    "        * E.g., breed data, temporal variables, whether this is the dog's first time at AAC, etc.\n",
    "        * Keep in mind though that some predictors will *unfairly* improve your prediction!\n",
    "            * Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "* Evaluating model performance is rarely straightforward\n",
    "* There are many criteria we might value\n",
    "* Simple answers can be misleading\n",
    "* Let's take a look at _how_ we classified different outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix\n",
    "* How does the classifier go wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The confusion_matrix is a utility that cross-tabulates the\n",
    "# classes assigned by our classifier with the ground truth\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y, y_predicted):\n",
    "    ''' take true and predicted scores and plot confusion matrix '''\n",
    "    # Get the confusion matrix\n",
    "    cm = confusion_matrix(y, y_predicted)\n",
    "\n",
    "    # Normalize the confusion matrix by dividing each row by its sum\n",
    "    ncm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Put into DataFrame and get all labels from the encoder\n",
    "    class_labels = encoder.classes_\n",
    "    ncm = pd.DataFrame(ncm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    # Rows are true classes, columns are assigned classes\n",
    "    sns.heatmap(data=ncm, fmt='.2f', annot=True, cmap='Blues', )\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plot_confusion_matrix(y, y_predicted)\n",
    "plt.gca().set_title('Confusion matrix\\nRow = reality, Column = decision', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The classification report shows us performance for\n",
    "# the most common metrics, by class\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y, y_predicted))\n",
    "print('Precision: Of all cases predicted to be X, how many really were X?')\n",
    "print('Recall: Of all cases that really are X, how many were correctly predicted to be X?\\n')\n",
    "\n",
    "# which integer goes with which outcome?\n",
    "# a cross-tab is a simple way to find out\n",
    "print(pd.crosstab(data['outcome'], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "* Hopefully performance now looks reasonable\n",
    "* But there's still a potential problem: overfitting\n",
    "* We're training and evaluating on the same dataset--this is a big no-no!\n",
    "* scikit-learn provides easy ways to evaluate models out-of-sample\n",
    "    * This is known as cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import KFold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Placeholder for scores from each fold\n",
    "scores = []\n",
    "\n",
    "# Create k folds (in our case 5). Loop over the folds,\n",
    "# and for each one, split the dataset into training and test.\n",
    "# In each fold, we train the data on the training values,\n",
    "# and then evaluate its performance on the test. Finally,\n",
    "# we can take the average of the out-of-sample scores as\n",
    "# our estimate of model performance.\n",
    "kf = KFold(5, shuffle=True)\n",
    "folds = kf.split(X)\n",
    "print(\"Indices of training/test samples in each fold:\")\n",
    "for train, test in folds:\n",
    "    # so we can see what's going on\n",
    "    print(train, test)\n",
    "    # We use the estimator from earlier, so make sure this\n",
    "    # is the classifier you want! Note that we are now\n",
    "    # only using the training indices to train.\n",
    "    est.fit(X[train], y[train])\n",
    "    # And similarly, we generate predictions only for the\n",
    "    # test indices. In this way, we never use the same\n",
    "    # data point for both training and testing.\n",
    "    pred_y = est.predict(X[test])\n",
    "    fold_score = accuracy_score(y[test], pred_y)\n",
    "    scores.append(fold_score)\n",
    "\n",
    "# Convert the list to an array and round\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Overfitting be gone!\n",
    "print(\"\\nAccuracy in the individual folds:\")\n",
    "print(scores.round(2))\n",
    "\n",
    "print(\"\\nOverall cross-validated accuracy:\")\n",
    "print(scores.mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Too much code?\n",
    "If you think the above is too much work, scikit-learn has you covered..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "est = RidgeClassifier()\n",
    "cross_val_score(est, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Model selection\n",
    "* We've experimented a lot with estimators, decomposition, evaluation, etc.\n",
    "* This is only the tip of the iceberg...\n",
    "* scikit-learn has hundreds of estimators!\n",
    "* Two problems:\n",
    "    1. How are we supposed to choose?\n",
    "    2. How do we do this in a principled way?\n",
    "* Basically we can try different things and see what leads to the highest cross-validation error\n",
    "* Caveat: *ALL* of the steps in our analysis pipeline...\n",
    "    * ...all data pre-processing strategies we try...\n",
    "    * ...all models we try...\n",
    "    * ...all hyperparameters (e.g., K parameter in KNN) we try for each model...\n",
    "* ...should ideally occur within another cross-validation loop, an idea called *nested cross-validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The learning curve\n",
    "* One helpful way to understand an estimator's behavior is to see how it scales with experience\n",
    "* As we increase the amount of data, how does performance evolve?\n",
    "    * In general, training performance will decrease, and test performance will increase\n",
    "    * The point of convergence is where we're no longer overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# The sample sizes at which we want to evaluate our estimator\n",
    "train_sizes = [500, 1000, 2000, 5000, 10000, 20000]\n",
    "\n",
    "# Get and assign the results.\n",
    "results = learning_curve(est, X, y, train_sizes=train_sizes,\n",
    "                         cv=5, shuffle=True)\n",
    "sizes, train_scores, test_scores = results\n",
    "\n",
    "# Let's look at the test scores... the rows are sample\n",
    "# sizes, and the columns are the folds of the k-folds procedure.\n",
    "# The training scores have exactly the same structure.\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the training and test scores simultaneously\n",
    "# to make things clearer\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "train_mean = train_scores.mean(1)\n",
    "test_mean = test_scores.mean(1)\n",
    "plt.plot(sizes, train_mean, 'o-', label='Training', lw=3)\n",
    "plt.plot(sizes, test_mean, 'o-', label='Test', lw=3);\n",
    "plt.xscale('log')\n",
    "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend(fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Sample size (n)', fontsize=14)\n",
    "plt.ylabel('$Accuracy$', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fully automated pipelines\n",
    "* We've done feature extraction, reduction, and selection; estimation; evaluation...\n",
    "* But we can automate this\n",
    "    * Both for efficiency, and to prevent overfitting (greatly facilitates nested cross-validation)\n",
    "* sklearn.pipeline provides functionality for creating [fully automated Pipelines](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "* We'll build a toy example with 2 steps, but we could chain our entire workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(k=100)\n",
    "# Model estimation\n",
    "estimator = LogisticRegression()\n",
    "\n",
    "# Put them together as the steps in a pipeline\n",
    "steps = [\n",
    "    ('select', selector),\n",
    "    ('estimate', estimator)\n",
    "]\n",
    "\n",
    "# Initialize the Pipeline with the steps\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# set up the predictors\n",
    "_X = np.c_[data['age'].values, lotsa_features]\n",
    "\n",
    "# standardize all predictors--i.e., subtract mean\n",
    "# and divide by standard deviation\n",
    "_X = scale(_X)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(_X, y)\n",
    "\n",
    "# Generate predictions\n",
    "_y_pred = pipeline.predict(_X)\n",
    "\n",
    "# Assess score\n",
    "accuracy_score(y, _y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building our own estimator:\n",
    "### The fit-predict interface\n",
    "* scikit-learn is built around the estimator interface\n",
    "* \"An estimator is an object that fits a model based on some training data and is capable of inferring some properties on new data\"\n",
    "* Every estimator must implement fit() and predict() methods\n",
    "* Makes it easy to extend scikit-learn with our own estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MercurialClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Picks a random class and assigns that label to all cases.\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Selects a random class from the available options '''\n",
    "        classes = np.unique(y)\n",
    "        self.selected_ = np.random.choice(classes)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Applies the selected class to everything '''\n",
    "        return np.repeat(self.selected_, len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-nearest neighbors (KNN) algorithm\n",
    "* Let's take a closer look at one model/algorithm in detail: **K-nearest neighbors (KNN)**\n",
    "    * Easy to understand\n",
    "    * Sometimes competitive with more complicated models\n",
    "    * Example of an algorithm really only used in machine learning context (no *data model*)\n",
    "<img src=\"images/knn.png\" style=\"margin-bottom: 10px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# some data pre-processing\n",
    "df = data[['min_height','min_weight']]\n",
    "df = (df - df.mean()) / df.std()                # standardize before applying PCA.\n",
    "df['size'] = np.dot(df, pca.components_.T)[:,0] # add PC1 as 'size'.\n",
    "df['outcome'] = data['outcome']                 # append outcome in string form\n",
    "encoder = LabelEncoder()                        # Initialize encoder for label conversion\n",
    "df['y'] = encoder.fit_transform(data['outcome'])# and integer form.\n",
    "df['age'] = data['age']                         # add the age predictor.\n",
    "df = df.dropna()                                # drop missing values.\n",
    "df = df.loc[df['outcome'] != 'Died', :]           # drop 'Died' outcome (too few cases, 0.4%)\n",
    "for v in ['age','size']:                        # standardize the predictors. KNN falters\n",
    "    df[v] = (df[v] - df[v].mean())/df[v].std()  # if predictors on wildly different scales.\n",
    "\n",
    "# Define outcome colors\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "# define function to compute KNN for a given K and plot the result\n",
    "def knn_plot(k, ax):\n",
    "    knn = neighbors.KNeighborsClassifier(k)     # load the KNN classifier\n",
    "    knn.fit(df[['size','age']], df['y'])        # fit the KNN model\n",
    "    predicted = knn.predict(df[['size','age']]) # retrieve the outcomes predicted by KNN\n",
    "    acc = accuracy_score(df['y'], predicted)    # compute accuracy of those predictions\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    h = .05  # step size in the mesh\n",
    "    x_min, x_max = df['size'].min() - 0.2, df['size'].max() + 0.2\n",
    "    y_min, y_max = df['age'].min() - 0.2, df['age'].max() + 0.2\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Create color maps from the lists\n",
    "    cmap = ListedColormap(colors)\n",
    "    # Lighter colors for the decision boundary\n",
    "    cmap_light = ListedColormap(['pink', 'lightgreen', 'lightblue', 'lightyellow'])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points--with some jittering added so we can\n",
    "    # see points a bit more clearly\n",
    "    n_obs = len(df['size'])\n",
    "    x_pt = df['size'] + np.random.normal(0, 0.05, n_obs)\n",
    "    y_pt = df['age'] + np.random.normal(0, 0.05, n_obs)\n",
    "    scattr = ax.scatter(x_pt, y_pt, c=df['y'], cmap=cmap, s=8)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Breed size (standardized)', fontsize=14)\n",
    "    ax.set_ylabel('Dog age (standardized)', fontsize=14)\n",
    "    ax.set_title('K = {}; Accuracy = {:.1%}'.format(k, acc), size=20)\n",
    "\n",
    "# Set our values of k\n",
    "k_values = [2, 100, 2000]\n",
    "n_k = len(k_values)\n",
    "\n",
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, n_k, figsize=(14, 14/n_k))\n",
    "\n",
    "# fit the models and make the plots\n",
    "for k, ax in zip(k_values, axes):\n",
    "    knn_plot(k, ax)\n",
    "    \n",
    "# Plot a custom legend\n",
    "patches = [mpatches.Patch(color=colors[i], label=encoder.classes_[i]) for i in range(4)]\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.03, 1.03), loc=2, fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A very quick dip into deep learning\n",
    "* What is deep learning?\n",
    "    * A field of machine learning that focuses on developing and applying \"deep\" neural network models\n",
    "* Why deep learning?\n",
    "    * The architecture of DNNs is (loosely) modeled on biological neural networks--which are very powerful!\n",
    "    * For many real-world tasks (image recognition, language translation, etc.), deep learning blows everything else out of the water\n",
    "    * A highly technical field\n",
    "    * But also a lot of trial-and-error\n",
    "    * Progress is *extremely* rapid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep learning in Python\n",
    "* Python is arguably the premier language for deep learning\n",
    "* Virtually all major frameworks (TensorFlow, Caffe, Torch, etc.) have Python bindings\n",
    "* High-level libraries like `keras` make things even easier\n",
    "* We don't have time to do the topic justice, so let's just see a quick example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Logistic regression with neural networks\n",
    "* Neural networks are extremely powerful function approximators\n",
    "* We can start by training a logistic regression model using a neural net architecture\n",
    "* Then we can start adding complexity (e.g., hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import keras as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our y is currently a 1-D array of integers; for multiclass classification,\n",
    "# keras's categorical_crossentropy loss expects a matrix of binary columns.\n",
    "# We use keras's to_categorical utility (which does essentially the same)\n",
    "# thing as pandas' get_dummies()) for the conversion.\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "# Set aside 20% of data as a hold-out test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2)\n",
    "\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "# # # # Multinomial logistic regression\n",
    "model = Sequential([\n",
    "    Dense(n_classes, activation='softmax', input_dim=X.shape[1])\n",
    "])\n",
    "\n",
    "# # Uncomment the next few lines for a deeper version of the model\n",
    "# # that contains a hidden layer\n",
    "model = Sequential([\n",
    "    Dense(50, activation='tanh', input_dim=X.shape[1]),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model using a categorical cross-entry loss\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Shuffle the training data\n",
    "perm = np.random.permutation(len(X_train))\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Fit the model!\n",
    "model.fit(X_train, y_train, nb_epoch=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing our network\n",
    "* Keras provides basic tools for visualizing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Going deeper...\n",
    "* If we improved performance non-trivially just by adding one hidden layer, shouldn't we just keep adding more layers?\n",
    "* In theory, perhaps, but in practice, probably not\n",
    "* Why not?\n",
    "    * Adding parameters makes the model harder to train, so we need more data\n",
    "    * There's probably a fundamental limit to how predictable outcomes are given these data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How did we do?\n",
    "* We can evaluate the neural network's performance using the same tools we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get predicted scores for the hold-out\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# The predictions are still in binary column format, so we need\n",
    "# to squash them back into a vector of integers of class IDs.\n",
    "# We can do this just by taking the max value in each row.\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = (y_test_class == y_pred_class).mean()\n",
    "print(\"Raw accuracy: {:.3f}\\n\".format(accuracy))\n",
    "\n",
    "names = encoder.classes_\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_test_class, y_pred_class, target_names=names))\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "plot_confusion_matrix(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End\n",
    "* I hope you enjoyed the course and/or learned something useful!\n",
    "* The links throughout the notebooks contain plenty of further resources\n",
    "* Consider registering for [SciPy 2018](https://scipy2018.scipy.org) (here in July)--they have great [tutorials](https://scipy2018.scipy.org/ehome/299527/711308/)\n",
    "* Feedback/suggestions for improvement is welcome\n",
    "* Please fill out a course evaluation before you leave"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
